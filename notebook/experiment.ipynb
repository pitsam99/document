{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cb272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467ef6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf05024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6946e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918d092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking, \"what is the capital of France.\" Let me start by recalling what I know about French geography. I remember that France is a country in Western Europe. The capital is a major city, probably one of the most well-known cities in the world. Let me think... I think it\\'s Paris. Yes, that sounds right. Paris is famous for landmarks like the Eiffel Tower and the Louvre. But wait, maybe I should double-check to make sure I\\'m not confusing it with another country\\'s capital. For example, Spain\\'s capital is Madrid, Italy\\'s is Rome, but France\\'s is definitely Paris. I don\\'t think there\\'s any confusion there. Let me just confirm in my mind. Paris is located in the northern part of France, along the Seine River. It\\'s a major cultural and economic hub. I\\'ve heard it referred to as the \"City of Light.\" Yeah, that\\'s right. So the answer should be Paris. I don\\'t think there\\'s any other city that\\'s commonly mistaken for the capital of France. Maybe some people might think Lyon or Marseille, but those are significant cities too, but not the capital. So the answer is Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is one of the most iconic and well-known cities in the world, famous for landmarks such as the Eiffel Tower, the Louvre Museum, and the Seine River. Paris is located in the ÃŽle-de-France region in northern France and serves as the country\\'s political, economic, and cultural center. ðŸ‡«ðŸ‡·âœ¨'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the capital of france\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e577f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3574b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eff24b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03458952158689499,\n",
       " -0.048770077526569366,\n",
       " -0.03808589279651642,\n",
       " -0.04295947030186653,\n",
       " 0.060769256204366684,\n",
       " -0.0004283846647012979,\n",
       " 0.0029317268636077642,\n",
       " -0.023212388157844543,\n",
       " 0.0034709786996245384,\n",
       " 0.07677333801984787,\n",
       " -0.014611724764108658,\n",
       " 0.002046124776825309,\n",
       " -0.004278103355318308,\n",
       " 0.030604152008891106,\n",
       " 0.024904100224375725,\n",
       " -0.03555342182517052,\n",
       " 0.001315358909778297,\n",
       " 0.035534635186195374,\n",
       " 0.04057428985834122,\n",
       " -0.011760558933019638,\n",
       " 0.018158266320824623,\n",
       " 0.006689310539513826,\n",
       " -0.0322573259472847,\n",
       " 0.026339339092373848,\n",
       " 0.01886705867946148,\n",
       " -0.06070532649755478,\n",
       " 0.007670633494853973,\n",
       " -0.05228906497359276,\n",
       " -0.01390334777534008,\n",
       " -0.007885364815592766,\n",
       " -0.07384371012449265,\n",
       " 0.045852918177843094,\n",
       " 0.007910694926977158,\n",
       " -0.011858788318932056,\n",
       " 0.021542945876717567,\n",
       " -0.0567542240023613,\n",
       " -0.004096544347703457,\n",
       " 0.01705259270966053,\n",
       " -0.006110567133873701,\n",
       " 0.037612058222293854,\n",
       " -0.010611383244395256,\n",
       " -0.0075986688025295734,\n",
       " -0.04854702576994896,\n",
       " 0.008374053984880447,\n",
       " -0.011739011853933334,\n",
       " -0.011546571739017963,\n",
       " -0.022393040359020233,\n",
       " 0.06501666456460953,\n",
       " 0.015506081283092499,\n",
       " -0.012837963178753853,\n",
       " 0.015925368294119835,\n",
       " -0.008150657638907433,\n",
       " 0.05198364332318306,\n",
       " 0.016386615112423897,\n",
       " 0.014652346260845661,\n",
       " -0.06117954105138779,\n",
       " 0.013528254814445972,\n",
       " -0.014014799147844315,\n",
       " -0.007525935769081116,\n",
       " 0.022473931312561035,\n",
       " 0.03699959069490433,\n",
       " -0.0261127520352602,\n",
       " 0.005898825824260712,\n",
       " 0.036067623645067215,\n",
       " 0.012799359858036041,\n",
       " -0.055872540920972824,\n",
       " 0.029513254761695862,\n",
       " 0.01729765348136425,\n",
       " 0.0759723111987114,\n",
       " 0.007619095034897327,\n",
       " -0.006765710189938545,\n",
       " -0.05015898868441582,\n",
       " 0.07419484108686447,\n",
       " 0.011664366349577904,\n",
       " 0.029476001858711243,\n",
       " -0.09371486306190491,\n",
       " -0.028986651450395584,\n",
       " 0.06947317719459534,\n",
       " 0.011830247938632965,\n",
       " -0.04833612218499184,\n",
       " -0.01547399815171957,\n",
       " -0.05540477856993675,\n",
       " -0.08617618680000305,\n",
       " -0.02892936021089554,\n",
       " -0.061803027987480164,\n",
       " 0.014498948119580746,\n",
       " -0.053320709615945816,\n",
       " -0.017274489626288414,\n",
       " -0.04264408349990845,\n",
       " 0.04802786558866501,\n",
       " -0.021607395261526108,\n",
       " 0.009294578805565834,\n",
       " 0.08005858212709427,\n",
       " -0.041630882769823074,\n",
       " -0.02953934669494629,\n",
       " 0.06519754230976105,\n",
       " -0.021383780986070633,\n",
       " -0.01191937830299139,\n",
       " 0.07132802158594131,\n",
       " -0.012650114484131336,\n",
       " 0.01890099048614502,\n",
       " -0.016409046947956085,\n",
       " -0.06665843725204468,\n",
       " 0.043104857206344604,\n",
       " 0.02989368513226509,\n",
       " -0.010624553076922894,\n",
       " -0.003977534361183643,\n",
       " 0.055732034146785736,\n",
       " 0.012131938710808754,\n",
       " 0.06812402606010437,\n",
       " -0.06923360377550125,\n",
       " -0.04456256330013275,\n",
       " 0.03155909478664398,\n",
       " 0.057239070534706116,\n",
       " 0.014815712347626686,\n",
       " -0.0494193397462368,\n",
       " -0.026240229606628418,\n",
       " 0.02090628258883953,\n",
       " 0.044752370566129684,\n",
       " 0.023283017799258232,\n",
       " 0.03834323212504387,\n",
       " -0.019856559112668037,\n",
       " 0.047199796885252,\n",
       " -0.04520058259367943,\n",
       " 0.03492969647049904,\n",
       " -0.017070617526769638,\n",
       " -0.04831460118293762,\n",
       " 0.03270692750811577,\n",
       " -0.0011523420689627528,\n",
       " 0.01608477719128132,\n",
       " 0.016140196472406387,\n",
       " -0.04194243624806404,\n",
       " -0.02545781619846821,\n",
       " -0.005125307012349367,\n",
       " 0.017819944769144058,\n",
       " 0.03919525071978569,\n",
       " 0.05327128991484642,\n",
       " -0.010310073383152485,\n",
       " 0.04114549979567528,\n",
       " 0.008479246869683266,\n",
       " 0.026273338124155998,\n",
       " 0.05891314893960953,\n",
       " 0.009933184832334518,\n",
       " 0.027952272444963455,\n",
       " -0.01736879162490368,\n",
       " 0.04947635158896446,\n",
       " -0.07385770976543427,\n",
       " -0.03269493207335472,\n",
       " 0.046924713999032974,\n",
       " -0.037726759910583496,\n",
       " -0.07384496927261353,\n",
       " 0.003137627150863409,\n",
       " -0.04826846718788147,\n",
       " -0.02916084975004196,\n",
       " 0.04151838272809982,\n",
       " 0.007005827035754919,\n",
       " -0.0069963992573320866,\n",
       " 0.050298307090997696,\n",
       " 0.02265380509197712,\n",
       " -0.013206996023654938,\n",
       " 0.05634894222021103,\n",
       " 0.014054067432880402,\n",
       " 0.016747232526540756,\n",
       " 0.007035024929791689,\n",
       " -0.0008641803287900984,\n",
       " -0.024381618946790695,\n",
       " 0.03325589373707771,\n",
       " -0.014852932654321194,\n",
       " 0.022384408861398697,\n",
       " -0.004099991172552109,\n",
       " -0.01495419442653656,\n",
       " 0.014960103668272495,\n",
       " -0.004454344976693392,\n",
       " -0.02401985228061676,\n",
       " 0.010145768523216248,\n",
       " -0.03529563173651695,\n",
       " 0.011396298184990883,\n",
       " -0.029290754348039627,\n",
       " -0.015150479972362518,\n",
       " -0.02733020670711994,\n",
       " 0.006588084157556295,\n",
       " -0.037016380578279495,\n",
       " 0.009269657544791698,\n",
       " 0.04155558720231056,\n",
       " 0.014305664226412773,\n",
       " -0.04653778299689293,\n",
       " 0.026035984978079796,\n",
       " -0.025531787425279617,\n",
       " 0.006719456985592842,\n",
       " 0.02373192273080349,\n",
       " -0.04934651404619217,\n",
       " -0.010832657106220722,\n",
       " -0.01993507519364357,\n",
       " -0.01813545450568199,\n",
       " -0.03767060115933418,\n",
       " -0.014276605099439621,\n",
       " 0.03628971800208092,\n",
       " -0.012479063123464584,\n",
       " 0.043113045394420624,\n",
       " -0.0517277829349041,\n",
       " -0.026351988315582275,\n",
       " 0.035172656178474426,\n",
       " 0.02618344873189926,\n",
       " -0.03312008082866669,\n",
       " 0.01809443160891533,\n",
       " -0.004294911399483681,\n",
       " 0.08886121213436127,\n",
       " -0.041425611823797226,\n",
       " -0.04798300936818123,\n",
       " 0.0547189898788929,\n",
       " -0.008952995762228966,\n",
       " 0.0067731780000030994,\n",
       " -0.020253287628293037,\n",
       " 0.007212336640805006,\n",
       " 0.037041567265987396,\n",
       " -0.02377067692577839,\n",
       " 0.07231511920690536,\n",
       " 0.009647779166698456,\n",
       " 0.04083578661084175,\n",
       " -0.025955302640795708,\n",
       " -0.04662148654460907,\n",
       " -0.005407457705587149,\n",
       " -0.029396722093224525,\n",
       " -0.012634249404072762,\n",
       " 0.03794257715344429,\n",
       " 0.030835092067718506,\n",
       " -0.0035221558064222336,\n",
       " -0.032035596668720245,\n",
       " 0.0008796353358775377,\n",
       " -0.013606003485620022,\n",
       " -0.046656444668769836,\n",
       " 0.07679469138383865,\n",
       " 0.028973035514354706,\n",
       " -0.021847855299711227,\n",
       " 0.06103310361504555,\n",
       " -0.003159484127536416,\n",
       " -0.00373261165805161,\n",
       " 0.01783609390258789,\n",
       " 0.0005455416394397616,\n",
       " 0.002943130210042,\n",
       " -0.03167610242962837,\n",
       " -0.03027423471212387,\n",
       " 0.047604940831661224,\n",
       " 0.02633526921272278,\n",
       " -0.04104382172226906,\n",
       " -0.03930028900504112,\n",
       " -0.025341397151350975,\n",
       " 0.022917380556464195,\n",
       " 0.044884707778692245,\n",
       " 0.04822775349020958,\n",
       " -0.006547174882143736,\n",
       " 0.0013844873756170273,\n",
       " 0.038297463208436966,\n",
       " 0.019187746569514275,\n",
       " -0.07419770210981369,\n",
       " 0.03227086737751961,\n",
       " -0.06976983696222305,\n",
       " 0.03045925684273243,\n",
       " -0.03587953373789787,\n",
       " 0.032846689224243164,\n",
       " 0.035096824169158936,\n",
       " 0.03097258321940899,\n",
       " 0.043393444269895554,\n",
       " -0.03457215055823326,\n",
       " -0.011816641315817833,\n",
       " -0.03694087639451027,\n",
       " -0.004430509638041258,\n",
       " -0.050621435046195984,\n",
       " 0.017136024311184883,\n",
       " 0.003853106638416648,\n",
       " 0.03273717314004898,\n",
       " -0.07278281450271606,\n",
       " 0.015396102331578732,\n",
       " 0.03509107604622841,\n",
       " 0.03265519440174103,\n",
       " 0.010217421688139439,\n",
       " -0.051406919956207275,\n",
       " 0.06394002586603165,\n",
       " 0.0655706450343132,\n",
       " -0.07094481587409973,\n",
       " 0.03676508739590645,\n",
       " 0.02342352084815502,\n",
       " -0.006888350937515497,\n",
       " -0.02145986817777157,\n",
       " -0.011488166637718678,\n",
       " -0.019053928554058075,\n",
       " -0.046588364988565445,\n",
       " -0.009769409894943237,\n",
       " 0.012427352368831635,\n",
       " -0.061935439705848694,\n",
       " -0.03428654372692108,\n",
       " -0.07760600000619888,\n",
       " 0.026752671226859093,\n",
       " -0.037495389580726624,\n",
       " -0.10199853032827377,\n",
       " -0.0017321689520031214,\n",
       " -0.027760908007621765,\n",
       " 0.03840360790491104,\n",
       " 0.03395644202828407,\n",
       " -0.021153906360268593,\n",
       " -0.028068426996469498,\n",
       " 0.006407698150724173,\n",
       " 0.0322730615735054,\n",
       " -0.06125568598508835,\n",
       " 0.02820236049592495,\n",
       " 0.032798781991004944,\n",
       " -0.038398243486881256,\n",
       " -0.060821641236543655,\n",
       " 0.026071134954690933,\n",
       " 0.02713744342327118,\n",
       " 0.04746917635202408,\n",
       " -0.017650354653596878,\n",
       " -0.06100231036543846,\n",
       " -0.03881692886352539,\n",
       " -0.016380872577428818,\n",
       " 0.06367835402488708,\n",
       " -0.02538864128291607,\n",
       " -0.021463997662067413,\n",
       " 0.021496709436178207,\n",
       " 0.02666313201189041,\n",
       " 0.01060787495225668,\n",
       " 0.07803975045681,\n",
       " 0.022656289860606194,\n",
       " -0.014647379517555237,\n",
       " -0.007651920430362225,\n",
       " 0.006413071416318417,\n",
       " 0.014938242733478546,\n",
       " 0.01246515940874815,\n",
       " 0.015075397677719593,\n",
       " -0.01718604564666748,\n",
       " -0.030822958797216415,\n",
       " 0.04139234870672226,\n",
       " -0.044337425380945206,\n",
       " 0.009038995020091534,\n",
       " 0.011801447719335556,\n",
       " 0.05920478329062462,\n",
       " -0.049942489713430405,\n",
       " 0.02620469033718109,\n",
       " -0.05003488436341286,\n",
       " 0.004067274741828442,\n",
       " 0.04555137827992439,\n",
       " 0.056303542107343674,\n",
       " -0.016706351190805435,\n",
       " -0.014966164715588093,\n",
       " -0.01665007695555687,\n",
       " -0.013483851216733456,\n",
       " -0.015690697357058525,\n",
       " 0.019079171121120453,\n",
       " 0.09224247932434082,\n",
       " 0.02608398161828518,\n",
       " 0.0007490787538699806,\n",
       " 0.07081658393144608,\n",
       " 0.008667184971272945,\n",
       " 0.04120778664946556,\n",
       " 0.0038913737516850233,\n",
       " -0.018644927069544792,\n",
       " 0.06384805589914322,\n",
       " -0.015410798601806164,\n",
       " 0.015197665430605412,\n",
       " -0.062449004501104355,\n",
       " 0.01712079346179962,\n",
       " 0.030798930674791336,\n",
       " -0.03754209354519844,\n",
       " -0.04824193939566612,\n",
       " -0.013524394482374191,\n",
       " -0.025823479518294334,\n",
       " -0.010111512616276741,\n",
       " -0.0005633183754980564,\n",
       " 0.018293725326657295,\n",
       " 0.018311601132154465,\n",
       " 0.015511809848248959,\n",
       " 0.01242170948535204,\n",
       " 0.0506642647087574,\n",
       " -0.04326286166906357,\n",
       " 0.05897916853427887,\n",
       " 0.028600940480828285,\n",
       " -0.06740537285804749,\n",
       " -0.01320361252874136,\n",
       " 0.018598027527332306,\n",
       " 0.02310841716825962,\n",
       " -0.04328496381640434,\n",
       " -0.02830282412469387,\n",
       " 0.04681311547756195,\n",
       " 0.031061610206961632,\n",
       " 0.01721740886569023,\n",
       " -0.022718362510204315,\n",
       " 0.04093173146247864,\n",
       " 0.010979223996400833,\n",
       " -0.028321105986833572,\n",
       " 0.052059270441532135,\n",
       " -0.0613284632563591,\n",
       " 0.07088986039161682,\n",
       " 0.0660267174243927,\n",
       " -0.022854240611195564,\n",
       " -0.01413289736956358,\n",
       " -0.02560199238359928,\n",
       " -0.0038106292486190796,\n",
       " -0.023968402296304703,\n",
       " 0.027961796149611473,\n",
       " 0.04246897995471954,\n",
       " -0.01692250929772854,\n",
       " -0.0710187703371048,\n",
       " -0.0373091958463192,\n",
       " -0.007009886205196381,\n",
       " -0.015244296751916409,\n",
       " 0.011980125680565834,\n",
       " 0.0005658547161146998,\n",
       " -0.006369052454829216,\n",
       " -0.05332774296402931,\n",
       " 0.016981283202767372,\n",
       " 0.008662475273013115,\n",
       " -0.018619947135448456,\n",
       " 0.013917667791247368,\n",
       " -0.0445600226521492,\n",
       " -0.015084809623658657,\n",
       " -0.0056523047387599945,\n",
       " 0.020885832607746124,\n",
       " -0.020423568785190582,\n",
       " -0.0016012353589758277,\n",
       " 0.06990475207567215,\n",
       " -0.01570061966776848,\n",
       " -0.009832533076405525,\n",
       " 0.01066379714757204,\n",
       " -0.01514962688088417,\n",
       " -0.05182058364152908,\n",
       " -0.06942585110664368,\n",
       " 0.0003721271059475839,\n",
       " 0.03842313587665558,\n",
       " 0.03564542159438133,\n",
       " -0.00879890937358141,\n",
       " 0.04290168732404709,\n",
       " 0.01930866949260235,\n",
       " -0.04539280757308006,\n",
       " -0.06298043578863144,\n",
       " -0.017419597133994102,\n",
       " -0.030628390610218048,\n",
       " 0.008331584744155407,\n",
       " 0.024940019473433495,\n",
       " -0.021519869565963745,\n",
       " -0.007790930103510618,\n",
       " 0.0075219725258648396,\n",
       " -0.043250955641269684,\n",
       " 0.03950825706124306,\n",
       " 0.022098436951637268,\n",
       " -0.07588112354278564,\n",
       " 0.004527947399765253,\n",
       " -0.015026652254164219,\n",
       " -0.015148747712373734,\n",
       " 0.009845957159996033,\n",
       " -0.0661383718252182,\n",
       " 0.026181502267718315,\n",
       " -0.026440393179655075,\n",
       " 0.0011816911865025759,\n",
       " -0.04435000941157341,\n",
       " -0.08797019720077515,\n",
       " -0.011443275958299637,\n",
       " -0.006431120447814465,\n",
       " 0.0812259316444397,\n",
       " -0.05621557682752609,\n",
       " 0.06539544463157654,\n",
       " 0.00588624645024538,\n",
       " -0.008337247185409069,\n",
       " -0.015443582087755203,\n",
       " -0.11476016044616699,\n",
       " 0.0733308419585228,\n",
       " 0.02625366486608982,\n",
       " 0.005263073369860649,\n",
       " -0.0038885707035660744,\n",
       " -0.014779307879507542,\n",
       " 0.030319124460220337,\n",
       " 0.02551039308309555,\n",
       " 0.0058546592481434345,\n",
       " -0.03193328529596329,\n",
       " -0.03985112905502319,\n",
       " -0.023123549297451973,\n",
       " -0.023845797404646873,\n",
       " -0.05697358772158623,\n",
       " 0.03907138854265213,\n",
       " -0.046295415610075,\n",
       " 0.013184661976993084,\n",
       " 0.012897682376205921,\n",
       " 0.018726451322436333,\n",
       " 0.01685524918138981,\n",
       " 0.0320688858628273,\n",
       " -0.02433469146490097,\n",
       " 0.03465178236365318,\n",
       " -0.018711792305111885,\n",
       " -0.011326639913022518,\n",
       " -0.05442791432142258,\n",
       " 0.02380123734474182,\n",
       " -0.002620947314426303,\n",
       " -0.023606421425938606,\n",
       " -0.016812549903988838,\n",
       " -0.012196722440421581,\n",
       " -0.017849614843726158,\n",
       " -0.012904634699225426,\n",
       " -0.000524963834322989,\n",
       " 0.04079167917370796,\n",
       " 0.035700324922800064,\n",
       " 0.0026067313738167286,\n",
       " -0.027799857780337334,\n",
       " -0.015948209911584854,\n",
       " -0.002880939980968833,\n",
       " -0.02188670076429844,\n",
       " 0.05802876502275467,\n",
       " -0.08782482147216797,\n",
       " -0.009090902283787727,\n",
       " 0.023471767082810402,\n",
       " -0.019458336755633354,\n",
       " -0.03282955288887024,\n",
       " 0.0033287820406258106,\n",
       " 0.013875916600227356,\n",
       " 0.0321008674800396,\n",
       " 0.043432604521512985,\n",
       " 0.02447391115128994,\n",
       " -0.0004611399199347943,\n",
       " 0.009142306633293629,\n",
       " -0.006720521487295628,\n",
       " 0.020328806713223457,\n",
       " -0.030663255602121353,\n",
       " 0.009046870283782482,\n",
       " -0.02615332417190075,\n",
       " -0.09807868301868439,\n",
       " -0.0009917900897562504,\n",
       " -0.027477465569972992,\n",
       " 0.01955411396920681,\n",
       " 0.027965456247329712,\n",
       " 0.05331447720527649,\n",
       " -0.03696587681770325,\n",
       " 0.0015927714994177222,\n",
       " -0.020539481192827225,\n",
       " 0.02338348515331745,\n",
       " -0.01298210397362709,\n",
       " -0.015463019721210003,\n",
       " 0.03211066499352455,\n",
       " -0.0045971074141561985,\n",
       " -0.008590532466769218,\n",
       " 0.01569202169775963,\n",
       " 0.022869998589158058,\n",
       " -0.023383518680930138,\n",
       " 0.0376591719686985,\n",
       " 0.023644259199500084,\n",
       " 0.06637553125619888,\n",
       " 0.021408455446362495,\n",
       " -0.02609100192785263,\n",
       " -0.017150072380900383,\n",
       " -0.005452724173665047,\n",
       " -0.05898111313581467,\n",
       " -0.019074464216828346,\n",
       " 0.03566895052790642,\n",
       " -0.005712667014449835,\n",
       " 0.016829850152134895,\n",
       " 0.008263693191111088,\n",
       " -0.01623685099184513,\n",
       " 0.01605733297765255,\n",
       " -0.024194110184907913,\n",
       " -0.01637042500078678,\n",
       " 0.04188772663474083,\n",
       " 0.013205979950726032,\n",
       " -0.04892487823963165,\n",
       " -0.04895976185798645,\n",
       " 0.006525156553834677,\n",
       " -0.009161839261651039,\n",
       " 0.023926835507154465,\n",
       " 0.08045326173305511,\n",
       " 0.028383910655975342,\n",
       " -0.013070804066956043,\n",
       " -0.017556406557559967,\n",
       " 0.06951991468667984,\n",
       " 0.004457312636077404,\n",
       " -0.01052387710660696,\n",
       " 0.014117591083049774,\n",
       " 0.005105394404381514,\n",
       " 0.02996629849076271,\n",
       " 0.00785331055521965,\n",
       " -0.02255621738731861,\n",
       " 0.008068046532571316,\n",
       " -0.03981934115290642,\n",
       " -0.04013831168413162,\n",
       " -0.013997809961438179,\n",
       " 0.029638009145855904,\n",
       " -0.006606375798583031,\n",
       " -0.002813722938299179,\n",
       " 0.019222993403673172,\n",
       " 0.01197232399135828,\n",
       " 0.031775183975696564,\n",
       " 0.05165477097034454,\n",
       " 0.10084284096956253,\n",
       " 0.04181237146258354,\n",
       " 0.05386144667863846,\n",
       " -0.035113625228405,\n",
       " 0.022715721279382706,\n",
       " -0.03264439105987549,\n",
       " -0.00029128146707080305,\n",
       " 0.016597768291831017,\n",
       " 0.0073625254444777966,\n",
       " -0.02064637467265129,\n",
       " -0.02842581272125244,\n",
       " -0.009209568612277508,\n",
       " -0.021348286420106888,\n",
       " 0.05619797855615616,\n",
       " -0.016910584643483162,\n",
       " 0.0009945282945409417,\n",
       " -0.020420223474502563,\n",
       " 0.01647375337779522,\n",
       " 0.06074732914566994,\n",
       " -0.001107106334529817,\n",
       " 0.006844908464699984,\n",
       " -0.00892356876283884,\n",
       " 0.01654129847884178,\n",
       " 0.02871437929570675,\n",
       " -0.0430111363530159,\n",
       " 0.008646639995276928,\n",
       " -0.017663074657320976,\n",
       " -0.02943105250597,\n",
       " -0.03554109111428261,\n",
       " 0.07003921270370483,\n",
       " 0.007398004177957773,\n",
       " 0.0194938275963068,\n",
       " -0.04221837967634201,\n",
       " 0.012626061215996742,\n",
       " -0.01424202136695385,\n",
       " 0.0345027893781662,\n",
       " -0.021829377859830856,\n",
       " 0.04637058824300766,\n",
       " 0.03458018600940704,\n",
       " 0.006234504748135805,\n",
       " -0.004974136129021645,\n",
       " 0.06807705760002136,\n",
       " 0.011494873091578484,\n",
       " 0.06184794753789902,\n",
       " 0.045370958745479584,\n",
       " -0.026734117418527603,\n",
       " 0.007766044232994318,\n",
       " -0.02832275815308094,\n",
       " -0.03775082528591156,\n",
       " -0.05481657013297081,\n",
       " -0.013188641518354416,\n",
       " 0.0064099375158548355,\n",
       " -0.022182544693350792,\n",
       " -0.03707096353173256,\n",
       " -0.020857330411672592,\n",
       " 0.0022248004097491503,\n",
       " -0.007645256817340851,\n",
       " 0.02450685203075409,\n",
       " 0.10827574133872986,\n",
       " 0.048209335654973984,\n",
       " -0.09761121869087219,\n",
       " -0.04767247661948204,\n",
       " -0.03411838412284851,\n",
       " -0.03394869342446327,\n",
       " -0.0030141803435981274,\n",
       " -0.003610546002164483,\n",
       " -0.021807018667459488,\n",
       " 0.05082292854785919,\n",
       " 0.023359112441539764,\n",
       " -0.02759222872555256,\n",
       " -0.05111955851316452,\n",
       " 0.017992280423641205,\n",
       " 0.015455668792128563,\n",
       " -0.015921488404273987,\n",
       " -0.02448376640677452,\n",
       " 0.012067331001162529,\n",
       " 0.008651570416986942,\n",
       " 0.008219707757234573,\n",
       " 0.019215047359466553,\n",
       " -0.026256749406456947,\n",
       " -0.07373441755771637,\n",
       " -0.009035530500113964,\n",
       " 0.049286894500255585,\n",
       " -0.0008273518178611994,\n",
       " 0.010844900272786617,\n",
       " 0.03084709495306015,\n",
       " 0.001613436732441187,\n",
       " 0.045031968504190445,\n",
       " 0.026506418362259865,\n",
       " -0.0013830768875777721,\n",
       " 0.018898626789450645,\n",
       " 0.01681404747068882,\n",
       " 0.011047372594475746,\n",
       " 0.0013676454545930028,\n",
       " -0.0015702953096479177,\n",
       " -0.017179546877741814,\n",
       " 0.004643936641514301,\n",
       " -0.033479880541563034,\n",
       " 0.05014437064528465,\n",
       " 0.03248756006360054,\n",
       " 0.012574080377817154,\n",
       " -0.03753969073295593,\n",
       " -0.04644903540611267,\n",
       " 0.00204398762434721,\n",
       " -0.01598172076046467,\n",
       " -0.06454885005950928,\n",
       " 0.0503612719476223,\n",
       " 0.06059011444449425,\n",
       " 0.021920328959822655,\n",
       " 0.032167814671993256,\n",
       " 0.0040933615528047085,\n",
       " -0.006570198107510805,\n",
       " 0.025578593835234642,\n",
       " -0.019077859818935394,\n",
       " -0.04017195850610733,\n",
       " -0.04358673468232155,\n",
       " 0.00321918330155313,\n",
       " -0.002303843619301915,\n",
       " 0.00041341892210766673,\n",
       " 0.03773579001426697,\n",
       " 0.036074310541152954,\n",
       " 0.05516126751899719,\n",
       " 0.0716981515288353,\n",
       " 0.036357879638671875,\n",
       " -0.022759336978197098,\n",
       " -0.0225614532828331,\n",
       " 0.023083370178937912,\n",
       " -0.013610742054879665,\n",
       " -0.052992165088653564,\n",
       " 0.028573937714099884,\n",
       " 0.036884855479002,\n",
       " -0.013961035758256912,\n",
       " 0.09983835369348526,\n",
       " 0.050151120871305466,\n",
       " 0.051421258598566055,\n",
       " 0.0619044154882431,\n",
       " -0.03092975541949272,\n",
       " -0.06008381024003029,\n",
       " 0.08159595727920532,\n",
       " -0.0897912010550499,\n",
       " -0.03369971737265587,\n",
       " -0.035668034106492996,\n",
       " 0.03761611878871918,\n",
       " 0.08502563834190369,\n",
       " -4.853308928431943e-05,\n",
       " -0.0014064586721360683,\n",
       " -0.015642240643501282,\n",
       " -0.0011522131972014904,\n",
       " 0.08606308698654175,\n",
       " 0.031055834144353867,\n",
       " 0.04318588972091675,\n",
       " -0.006004945375025272,\n",
       " -0.08886055648326874,\n",
       " -0.022410623729228973,\n",
       " 0.036051906645298004,\n",
       " 0.01315268687903881,\n",
       " 0.04163098707795143,\n",
       " 0.02600865252315998,\n",
       " 0.001692421268671751,\n",
       " -0.013852108269929886,\n",
       " -0.035840608179569244,\n",
       " 0.025168688967823982,\n",
       " -0.07334113121032715,\n",
       " -0.037229008972644806,\n",
       " 0.005657326430082321,\n",
       " -0.031244518235325813,\n",
       " 0.035917289555072784,\n",
       " 0.03470577672123909,\n",
       " -0.01201153825968504,\n",
       " -0.01882193610072136,\n",
       " 0.020705517381429672,\n",
       " -0.011294513009488583,\n",
       " 0.004732941742986441,\n",
       " -0.0196756049990654,\n",
       " -0.045209646224975586,\n",
       " 0.0021003272850066423,\n",
       " 0.007033917121589184,\n",
       " 0.005702544003725052,\n",
       " 0.01569640263915062,\n",
       " 0.04731985926628113,\n",
       " -0.0072886343114078045]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.embed_query(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b21bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a99cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02574853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0e959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(), \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0115bcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c269fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "73ab76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "26cdc8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "975c1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c3079920",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb960557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "86439d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€ \\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "902ae610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a3f578d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€ \\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b60912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0ebd4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore= FAISS.from_documents(doc, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "392aa639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2c9553d6920>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "91e24524",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"The capabilities of LLMs are remarkable considering the seemingly straightforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa770610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0c10cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "879114d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='57851be2-73cb-49d6-942e-a7f9b112d560', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='2d6ea6d6-b816-4923-9192-bb962a9d0e2a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='2016b07e-3f5f-4d7e-b057-6eafedf836f8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='f357bf3d-fbfd-47ce-8e86-5b30f1f80956', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='215710d7-d9c8-4bce-8d4e-e0832cd16f24', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='0f429a5e-1621-49e4-b2ef-9ae98ad16355', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Human-Eval and MBPP respectively. For pass@100 and pass@80 scores, we use a temperature of 0.8 and\\ntop-p=0.95. For pass@1 scores, we use a temperature of 0.1 and top-p=0.95.\\n49'),\n",
       " Document(id='aedd73e9-588c-417e-b5b7-4bac4c9c6a32', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9'),\n",
       " Document(id='4d5ee47d-c28a-48a7-a05a-72612fb7c02e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 1\\n7B 0.27 0.26 0.34 0.54 0.36 0.39 0.26 0.28 0.33 0.45 0.33 0.17 0.24 0.31 0.44 0.57 0.39 0.3513B 0.24 0.24 0.31 0.52 0.37 0.37 0.23 0.28 0.31 0.50 0.27 0.10 0.24 0.27 0.41 0.55 0.34 0.2533B 0.23 0.26 0.34 0.50 0.36 0.35 0.24 0.33 0.34 0.49 0.31 0.12 0.23 0.30 0.41 0.60 0.28 0.2765B 0.25 0.26 0.34 0.46 0.36 0.40 0.25 0.32 0.32 0.48 0.31 0.11 0.25 0.30 0.43 0.60 0.39 0.34\\nLlama 2'),\n",
       " Document(id='7bf2bea0-6e4f-486d-88fe-0cc97d726a91', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 72, 'page_label': '73'}, page_content='Llama 2\\n7B 0.15 0.30 0.12 0.35 0.25 0.43 0.18 0.38 0.16 0.12 0.29 -0.1313B 0.14 0.35 0.23 0.29 0.23 0.57 0.20 0.52 0.22 0.12 0.29 -0.1734B 0.12 0.16 0.18 0.36 0.35 0.52 0.10 0.54 0.28 0.11 0.30 -0.1970B 0.16 0.21 0.17 0.35 0.30 0.60 0.18 0.67 0.26 0.12 0.30 -0.10\\nFine-tuned\\nChatGPT 0.15 0.22 0.05 0.24 0.31 0.35 0.09 0.42 0.19 0.09 0.23 0.06MPT-instruct 7B 0.13 0.29 0.12 0.34 0.35 0.53 0.28 0.56 0.27 0.02 0.32 -0.12Falcon-instruct 7B 0.11 0.21 0.21 0.28 0.34 0.23 0.31 0.45 0.23 0.22 0.29 -0.27'),\n",
       " Document(id='414f6003-2e0e-4ce4-a818-73845919ff4a', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\AI_agent\\\\document\\\\notebook\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='be on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690481f",
   "metadata": {},
   "source": [
    "### Context: based on the question retrieving the info from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "33cfd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d32f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3807edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59afa898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b4e7f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "11de1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "13cf3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5b32af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "925af53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4051d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dc37b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, I need to answer the question about Llama 2 fine-tuning benchmark experiments based on the provided context. Let me go through the context step by step to find relevant information.\\n\\nFirst, I\\'ll look for any sections or tables that mention \"fine-tuned\" or \"Llama 2 fine-tuning.\" Scanning through the context, I see a few mentions. There\\'s a table labeled \"Table 3\" which discusses overall performance on grouped academic benchmarks, including a line for \"Fine-tuned ChatGPT\" and \"Llama 2\" models with different sizes (7B, 13B, 34B, 70B). This table has multiple columns with numbers, which might represent various benchmarks or metrics.\\n\\nFurther down, there\\'s another section with \"Fine-tuned\" models, including ChatGPT and Llama 2 variants like \"Llama 2-Chat.\" The numbers here seem to be performance scores, possibly accuracy or another metric across different benchmarks.\\n\\nI also notice a part where \"ablation experiments\" are mentioned, which might relate to fine-tuning. Additionally, there\\'s a table near the end showing percentages for \"true,\" \"info,\" and another metric, with rows for \"Fine-tuned ChatGPT\" and other models. This could indicate performance improvements after fine-tuning.\\n\\nIn the context, it\\'s mentioned that for fine-tuned models, they pick the best score between their evaluation framework and any publicly reported results. This suggests that the benchmark experiments involve comparing the models\\' performance after fine-tuning on specific tasks or datasets.\\n\\nThe hyperparameters section details the training setup, which is crucial for understanding how fine-tuning was done. They used the AdamW optimizer with specific beta values, a cosine learning rate schedule, warmup steps, and weight decay. This setup would affect the fine-tuning process and the resulting benchmark performance.\\n\\nPutting this together, the benchmark experiments for Llama 2 fine-tuning likely involved evaluating the models on a suite of academic benchmarks, comparing them against other models like ChatGPT, and using optimized training parameters. The results show improved performance across various metrics, with higher scores in certain categories for the fine-tuned Llama 2 models compared to their pretrained counterparts.\\n\\nI should structure the answer to include the benchmarks used, the models compared, the evaluation framework, and the specific performance metrics mentioned. Also, mentioning the hyperparameters provides context on how the fine-tuning was conducted.\\n\\nI need to make sure I don\\'t include any information beyond what\\'s provided. If any part is unclear or not covered, I should avoid speculation and stick to the data given.\\n</think>\\n\\nThe Llama 2 fine-tuning benchmark experiments involved evaluating the model on a suite of academic benchmarks, comparing it against other models such as ChatGPT. The fine-tuned Llama 2 models demonstrated improved performance across various metrics, with specific scores highlighted in the provided context. The evaluation framework considered both internal results and publicly reported data, ensuring the best scores were selected. Training parameters included the use of the AdamW optimizer with specific beta values, a cosine learning rate schedule, warmup steps, and weight decay, which contributed to the models\\' performance improvements. The experiments showcased the effectiveness of fine-tuning, with detailed results available in the context.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f9432a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out the scaling trends for the reward model based on the provided context. Let me read through the context carefully.\\n\\nFirst, I see a mention of \"larger models obtain higher performance for a similar volume of data.\" That suggests that as the model size increases, the performance improves, given the same amount of data. So scaling up the model leads to better results.\\n\\nThen, it says the scaling performance hasn\\'t plateaued yet, which means even with the current data, the models can still improve. This indicates that there\\'s room for further growth if more data is added through annotations. So, more data could lead to even better performance.\\n\\nAdditionally, there\\'s a discussion about the reward model accuracy and how it\\'s one of the most important factors. This implies that improving the model\\'s ability to accurately assign rewards is crucial for overall performance.\\n\\nLooking at Figure 6, it\\'s mentioned that it reports these trends, showing larger models perform better. So, the scaling trend is positiveâ€”bigger models do better with the same data volume.\\n\\nI don\\'t see any negative trends mentioned here, just that the potential for improvement is still there. So, the scaling trend is that increasing model size leads to higher performance without having reached the limit yet.\\n</think>\\n\\nThe scaling trends for the reward model indicate that larger models achieve higher performance with a similar volume of data. The performance has not yet plateaued, suggesting there is potential for further improvement with additional data annotations. This positive trend implies that scaling up the model size continues to enhance performance effectively.'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me Scaling trends for the reward model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fb28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
